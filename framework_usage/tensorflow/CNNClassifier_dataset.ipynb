{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 那些事儿之DL中的 HELLO WORLD\n",
    "\n",
    "- 基于MNIST数据集，运用TensorFlow中的 **tf.estimator** 中的 **tf.estimator.Estimator** 搭建一个简单的卷积神经网络，实现模型的训练，验证和测试\n",
    "\n",
    "- TensorBoard的简单使用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入各个库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "from tensorflow import data\n",
    "from tensorflow.python.feature_column import feature_column\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST数据集载入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看看MNIST数据长什么样子的\n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "More info: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MNIST数据集包含70000张图像和对应的标签（图像的分类）。数据集被划为3个子集：训练集，验证集和测试集。\n",
    "\n",
    "- 定义**MNIST**数据的相关信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_FILES_PATTERN = 'data_csv/mnist_train.csv'\n",
    "VAL_DATA_FILES_PATTERN = 'data_csv/mnist_val.csv'\n",
    "TEST_DATA_FILES_PATTERN = 'data_csv/mnist_test.csv'\n",
    "\n",
    "MULTI_THREADING = True\n",
    "RESUME_TRAINING = False\n",
    "\n",
    "NUM_CLASS = 10\n",
    "IMG_SHAPE = [28,28]\n",
    "\n",
    "IMG_WIDTH = 28\n",
    "IMG_HEIGHT = 28\n",
    "IMG_FLAT = 784\n",
    "NUM_CHANNEL = 1\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_TRAIN = 55000\n",
    "NUM_VAL = 5000\n",
    "NUM_TEST = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data (10000, 784)\n",
      "test_label (10000,)\n",
      "val_data (5000, 784)\n",
      "val_label (5000,)\n",
      "train_data (55000, 784)\n",
      "train_label (55000,)\n"
     ]
    }
   ],
   "source": [
    "# train_data = pd.read_csv(TRAIN_DATA_FILES_PATTERN)\n",
    "# train_data = pd.read_csv(TRAIN_DATA_FILES_PATTERN, header=None, names=HEADER )\n",
    "train_data = pd.read_csv(TRAIN_DATA_FILES_PATTERN, header=None)\n",
    "test_data = pd.read_csv(TEST_DATA_FILES_PATTERN, header=None)\n",
    "val_data = pd.read_csv(VAL_DATA_FILES_PATTERN, header=None)\n",
    "\n",
    "train_values = train_data.values\n",
    "train_data = train_values[:,1:]/255.0\n",
    "train_label = train_values[:,0:1].squeeze()\n",
    "\n",
    "val_values = val_data.values\n",
    "val_data = val_values[:,1:]/255.0\n",
    "val_label = val_values[:,0:1].squeeze()\n",
    "\n",
    "test_values = test_data.values\n",
    "test_data = test_values[:,1:]/255.0\n",
    "test_label = test_values[:,0:1].squeeze()\n",
    "\n",
    "print('test_data',np.shape(test_data))\n",
    "print('test_label',np.shape(test_label))\n",
    "\n",
    "print('val_data',np.shape(val_data))\n",
    "print('val_label',np.shape(val_label))\n",
    "\n",
    "print('train_data',np.shape(train_data))\n",
    "print('train_label',np.shape(train_label))\n",
    "\n",
    "# train_data.head(10)\n",
    "# test_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 试试自己写一个estimator\n",
    "\n",
    "- 基于MNIST数据集，运用TensorFlow中的 **tf.estimator** 中的 **tf.estimator.Estimator** 搭建一个简单的卷积神经网络，实现模型的训练，验证和测试\n",
    "\n",
    "- [官网API](https://tensorflow.google.cn/api_docs/python/tf/estimator/Estimator)\n",
    "\n",
    "- 看看有哪些参数\n",
    "\n",
    "```python\n",
    "__init__(\n",
    "    model_fn,\n",
    "    model_dir=None,\n",
    "    config=None,\n",
    "    params=None,\n",
    "    warm_start_from=None\n",
    ")\n",
    "```\n",
    "- 本例中，主要用了 **tf.estimator.Estimator** 中的 `model_fn`,`model_dir`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先简单看看数据流\n",
    "\n",
    "下面的图表直接显示了本次MNIST例子的数据流向，共有**2个卷积层**，每一层卷积之后采用最大池化进行下采样（图中并未画出），最后接**2个全连接层**，实现对MNIST数据集的分类\n",
    "\n",
    "![Flowchart](images/02_network_flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先看看input_fn之创建输入函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate tf.data.TextLineDataset() using make_one_shot_iterator()\n",
    "\n",
    "def decode_line(line):\n",
    "    # Decode the csv_line to tensor.\n",
    "    record_defaults = [[1.0] for col in range(785)]\n",
    "    items = tf.decode_csv(line, record_defaults)\n",
    "    features = items[1:785]\n",
    "    label = items[0]\n",
    "\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    features = tf.reshape(features,[28,28,1])\n",
    "    features = tf.image.flip_left_right(features)\n",
    "#     print('features_aug',features_aug)\n",
    "    label = tf.cast(label, tf.int64)\n",
    "#     label = tf.one_hot(label,num_class)\n",
    "    return features,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_input_fn(files_name_pattern, mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                 skip_header_lines=1, \n",
    "                 num_epochs=None, \n",
    "                 batch_size=128):\n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "        \n",
    "    num_threads = multiprocessing.cpu_count() if MULTI_THREADING else 1\n",
    "     \n",
    "    print(\"\")\n",
    "    print(\"* data input_fn:\")\n",
    "    print(\"================\")\n",
    "    print(\"Input file(s): {}\".format(files_name_pattern))\n",
    "    print(\"Batch size: {}\".format(batch_size))\n",
    "    print(\"Epoch Count: {}\".format(num_epochs))\n",
    "    print(\"Mode: {}\".format(mode))\n",
    "    print(\"Thread Count: {}\".format(num_threads))\n",
    "    print(\"Shuffle: {}\".format(shuffle))\n",
    "    print(\"================\")\n",
    "    print(\"\")\n",
    "\n",
    "    file_names = tf.matching_files(files_name_pattern)\n",
    "    dataset = data.TextLineDataset(filenames=file_names).skip(1)\n",
    "#     dataset = tf.data.TextLineDataset(filenames).skip(1)\n",
    "    print(\"DATASET\",dataset)\n",
    "\n",
    "    # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "    # tensor for each example.\n",
    "    dataset = dataset.map(decode_line)\n",
    "    print(\"DATASET_1\",dataset)\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    print(\"DATASET_2\",dataset)\n",
    "    dataset = dataset.batch(32)\n",
    "    print(\"DATASET_3\",dataset)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    print(\"DATASET_4\",dataset)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    # `features` is a dictionary in which each value is a batch of values for\n",
    "    # that feature; `labels` is a batch of labels.\n",
    "    features, labels = iterator.get_next()\n",
    "    \n",
    "    features = {'images':features}\n",
    "    \n",
    "    return features,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file(s): data_csv/mnist_train.csv\n",
      "Batch size: 128\n",
      "Epoch Count: None\n",
      "Mode: train\n",
      "Thread Count: 6\n",
      "Shuffle: True\n",
      "================\n",
      "\n",
      "DATASET <DatasetV1Adapter shapes: (), types: tf.string>\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "DATASET_1 <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_2 <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_3 <DatasetV1Adapter shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "DATASET_4 <DatasetV1Adapter shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "Features in CSV: ['images']\n",
      "Target in CSV: Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "features, target = csv_input_fn(files_name_pattern=TRAIN_DATA_FILES_PATTERN)\n",
    "print(\"Features in CSV: {}\".format(list(features.keys())))\n",
    "print(\"Target in CSV: {}\".format(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_x = tf.feature_column.numeric_column('images', shape=IMG_SHAPE)\n",
    "\n",
    "feature_columns = [feature_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重点在这里——model_fn\n",
    "\n",
    "\n",
    "#### model_fn: Model function. Follows the signature:\n",
    "\n",
    "* Args:\n",
    "  * `features`: This is the first item returned from the `input_fn` passed to `train`, `evaluate`, and `predict`. This should be a single `tf.Tensor` or `dict` of same.\n",
    "  * `labels`: This is the second item returned from the `input_fn` passed to `train`, `evaluate`, and `predict`. This should be a single `tf.Tensor` or `dict` of same (for multi-head models).If mode is @{tf.estimator.ModeKeys.PREDICT}, `labels=None` will be passed. If the `model_fn`'s signature does not accept `mode`, the `model_fn` must still be able to handle `labels=None`.\n",
    "  * `mode`: Optional. Specifies if this training, evaluation or prediction. See `tf.estimator.ModeKeys`.\n",
    "  * `params`: Optional `dict` of hyperparameters.  Will receive what is passed to Estimator in `params` parameter. This allows to configure Estimators from hyper parameter tuning.\n",
    "  * `config`: Optional `estimator.RunConfig` object. Will receive what is passed to Estimator as its `config` parameter, or a default value. Allows setting up things in your `model_fn` based on configuration such as `num_ps_replicas`, or `model_dir`.\n",
    "* Returns:\n",
    "    `tf.estimator.EstimatorSpec`\n",
    "    \n",
    "#### 注意model_fn返回的tf.estimator.EstimatorSpec\n",
    "<img style=\"float: left;\" src=\"images/0_TF_HELLO.png\" width=\"60%\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义我们自己的model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    # Args:\n",
    "    #\n",
    "    # features: This is the x-arg from the input_fn.\n",
    "    # labels:   This is the y-arg from the input_fn,\n",
    "    #           see e.g. train_input_fn for these two.\n",
    "    # mode:     Either TRAIN, EVAL, or PREDICT\n",
    "    # params:   User-defined hyper-parameters, e.g. learning-rate.\n",
    "    \n",
    "    # Reference to the tensor named \"x\" in the input-function.\n",
    "#     x = features[\"images\"]\n",
    "    x = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    # The convolutional layers expect 4-rank tensors\n",
    "    # but x is a 2-rank tensor, so reshape it.\n",
    "    net = tf.reshape(x, [-1, IMG_HEIGHT, IMG_WIDTH, NUM_CHANNEL])    \n",
    "\n",
    "    # First convolutional layer.\n",
    "    net = tf.layers.conv2d(inputs=net, name='layer_conv1',\n",
    "                           filters=16, kernel_size=5,\n",
    "                           padding='same', activation=tf.nn.relu)\n",
    "    net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n",
    "\n",
    "    # Second convolutional layer.\n",
    "    net = tf.layers.conv2d(inputs=net, name='layer_conv2',\n",
    "                           filters=36, kernel_size=5,\n",
    "                           padding='same', activation=tf.nn.relu)\n",
    "    net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)    \n",
    "\n",
    "    # Flatten to a 2-rank tensor.\n",
    "    net = tf.contrib.layers.flatten(net)\n",
    "    # Eventually this should be replaced with:\n",
    "    # net = tf.layers.flatten(net)\n",
    "\n",
    "    # First fully-connected / dense layer.\n",
    "    # This uses the ReLU activation function.\n",
    "    net = tf.layers.dense(inputs=net, name='layer_fc1',\n",
    "                          units=128, activation=tf.nn.relu)    \n",
    "\n",
    "    # Second fully-connected / dense layer.\n",
    "    # This is the last layer so it does not use an activation function.\n",
    "    net = tf.layers.dense(inputs=net, name='layer_fc2',\n",
    "                          units=10)\n",
    "\n",
    "    # Logits output of the neural network.\n",
    "    logits = net\n",
    "\n",
    "    # Softmax output of the neural network.\n",
    "    y_pred = tf.nn.softmax(logits=logits)\n",
    "    \n",
    "    # Classification output of the neural network.\n",
    "    y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # If the estimator is supposed to be in prediction-mode\n",
    "        # then use the predicted class-number that is output by\n",
    "        # the neural network. Optimization etc. is not needed.\n",
    "        spec = tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=y_pred_cls)\n",
    "    else:\n",
    "        # Otherwise the estimator is supposed to be in either\n",
    "        # training or evaluation-mode. Note that the loss-function\n",
    "        # is also required in Evaluation mode.\n",
    "        \n",
    "        # Define the loss-function to be optimized, by first\n",
    "        # calculating the cross-entropy between the output of\n",
    "        # the neural network and the true labels for the input data.\n",
    "        # This gives the cross-entropy for each image in the batch.\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                                       logits=logits)\n",
    "\n",
    "        # Reduce the cross-entropy batch-tensor to a single number\n",
    "        # which can be used in optimization of the neural network.\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "        # Define the optimizer for improving the neural network.\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=params[\"learning_rate\"])\n",
    "\n",
    "        # Get the TensorFlow op for doing a single optimization step.\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Define the evaluation metrics,\n",
    "        # in this case the classification accuracy.\n",
    "        metrics = \\\n",
    "        {\n",
    "            \"accuracy\": tf.metrics.accuracy(labels, y_pred_cls)\n",
    "        }\n",
    "\n",
    "        # Wrap all of this in an EstimatorSpec.\n",
    "        spec = tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops=metrics)\n",
    "        \n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自建的estimator在这里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"learning_rate\": 1e-4,\n",
    "         'feature_columns': feature_columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_evaluation_master': '', '_model_dir': '/home/moDisk/Models/mnist/cnn_dataset', '_device_fn': None, '_eval_distribute': None, '_protocol': None, '_task_id': 0, '_save_checkpoints_secs': 600, '_train_distribute': None, '_is_chief': True, '_num_worker_replicas': 1, '_experimental_distribute': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_task_type': 'worker', '_master': '', '_global_id_in_cluster': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3b0ec4f940>, '_keep_checkpoint_max': 5, '_num_ps_replicas': 0, '_save_summary_steps': 100, '_tf_random_seed': None}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                               params=params,\n",
    "                               model_dir=\"/home/moDisk/Models/mnist/cnn_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练训练看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file(s): data_csv/mnist_train.csv\n",
      "Batch size: 128\n",
      "Epoch Count: None\n",
      "Mode: train\n",
      "Thread Count: 6\n",
      "Shuffle: True\n",
      "================\n",
      "\n",
      "DATASET <DatasetV1Adapter shapes: (), types: tf.string>\n",
      "DATASET_1 <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_2 <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_3 <DatasetV1Adapter shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "DATASET_4 <DatasetV1Adapter shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/moDisk/Models/mnist/cnn_dataset/model.ckpt-6000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 6000 into /home/moDisk/Models/mnist/cnn_dataset/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.005418867, step = 6000\n",
      "INFO:tensorflow:global_step/sec: 41.22\n",
      "INFO:tensorflow:loss = 0.0013214283, step = 6100 (2.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.7253\n",
      "INFO:tensorflow:loss = 0.0053536785, step = 6200 (2.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.7668\n",
      "INFO:tensorflow:loss = 0.0026322831, step = 6300 (2.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.8796\n",
      "INFO:tensorflow:loss = 0.0014341163, step = 6400 (2.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.9493\n",
      "INFO:tensorflow:loss = 0.09013666, step = 6500 (2.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.6976\n",
      "INFO:tensorflow:loss = 0.06982586, step = 6600 (2.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.5179\n",
      "INFO:tensorflow:loss = 0.00084869534, step = 6700 (2.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.9295\n",
      "INFO:tensorflow:loss = 0.18341301, step = 6800 (2.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.653\n",
      "INFO:tensorflow:loss = 0.028597914, step = 6900 (2.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.7583\n",
      "INFO:tensorflow:loss = 0.00057957345, step = 7000 (2.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.3965\n",
      "INFO:tensorflow:loss = 0.12487179, step = 7100 (2.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.4475\n",
      "INFO:tensorflow:loss = 0.0263134, step = 7200 (2.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.0978\n",
      "INFO:tensorflow:loss = 0.002285563, step = 7300 (2.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.3\n",
      "INFO:tensorflow:loss = 0.024913989, step = 7400 (2.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.26\n",
      "INFO:tensorflow:loss = 0.060334258, step = 7500 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 367.103\n",
      "INFO:tensorflow:loss = 0.020868696, step = 7600 (0.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 377.516\n",
      "INFO:tensorflow:loss = 0.001674233, step = 7700 (0.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.08\n",
      "INFO:tensorflow:loss = 0.011468912, step = 7800 (7.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.7691\n",
      "INFO:tensorflow:loss = 0.007498665, step = 7900 (2.284 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8000 into /home/moDisk/Models/mnist/cnn_dataset/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.028672617.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f3b0ec4fc88>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fn = lambda: csv_input_fn(\\\n",
    "                                files_name_pattern= TRAIN_DATA_FILES_PATTERN,mode=tf.estimator.ModeKeys.TRAIN)\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证一下瞅瞅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file(s): data_csv/mnist_val.csv\n",
      "Batch size: 128\n",
      "Epoch Count: None\n",
      "Mode: eval\n",
      "Thread Count: 6\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "DATASET <DatasetV1Adapter shapes: (), types: tf.string>\n",
      "DATASET_1 <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_2 <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_3 <DatasetV1Adapter shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "DATASET_4 <DatasetV1Adapter shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-03T02:57:07Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/moDisk/Models/mnist/cnn_dataset/model.ckpt-6000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-03-02:57:10\n",
      "INFO:tensorflow:Saving dict for global step 6000: accuracy = 0.96875, global_step = 6000, loss = 0.15578994\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6000: /home/moDisk/Models/mnist/cnn_dataset/model.ckpt-6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.96875, 'global_step': 6000, 'loss': 0.15578994}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fn = lambda: csv_input_fn(files_name_pattern= VAL_DATA_FILES_PATTERN,mode=tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "model.evaluate(input_fn,steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* data input_fn:\n",
      "================\n",
      "Input file(s): data_csv/mnist_test.csv\n",
      "Batch size: 10\n",
      "Epoch Count: None\n",
      "Mode: infer\n",
      "Thread Count: 6\n",
      "Shuffle: False\n",
      "================\n",
      "\n",
      "DATASET <DatasetV1Adapter shapes: (), types: tf.string>\n",
      "DATASET_1 <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_2 <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.float32, tf.int64)>\n",
      "DATASET_3 <DatasetV1Adapter shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "DATASET_4 <DatasetV1Adapter shapes: ((?, 28, 28, 1), (?,)), types: (tf.float32, tf.int64)>\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /home/moDisk/Models/mnist/cnn_dataset/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "PREDICTIONS [3, 5, 0, 2, 3, 4, 7, 9, 7, 3]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "input_fn = lambda: csv_input_fn(\\\n",
    "                                files_name_pattern= TEST_DATA_FILES_PATTERN,mode=tf.estimator.ModeKeys.PREDICT,batch_size=10)\n",
    "\n",
    "predictions = list(itertools.islice(model.predict(input_fn=input_fn),10))\n",
    "print('PREDICTIONS',predictions)\n",
    "# print(\"\")\n",
    "# print(\"* Predicted Classes: {}\".format(list(map(lambda item: item[\"classes\"][0]\n",
    "#     ,predictions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
